{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "# importing libraries to plot the wordcloud\n",
    "#from wordcloud import WordCloud, STOPWORDS\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "local = \"São Paulo\"\n",
    "email = 'caiocezar05@gmail.com'\n",
    "senha = 'psx36547'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def login(email,senha):\n",
    "    driver = webdriver.Chrome(r\"C:\\Users\\a92550\\Downloads\\chromedriver.exe\")\n",
    "    driver.get('https://www.linkedin.com/login')\n",
    "    # waiting load\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Search for login and password inputs, send credentions\n",
    "    driver.find_element_by_id('username').send_keys(email)\n",
    "    driver.find_element_by_id('password').send_keys(senha)\n",
    "    try:\n",
    "        driver.find_element_by_XPATH('/html/body/div/div[1]/section/div[2]/div/article/footer/div/div/button[1]').click()\n",
    "    except: pass\n",
    "\n",
    "    driver.find_element_by_id('password').send_keys(Keys.RETURN)\n",
    "    return driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def job_Search_complete(driver,cargo,local,npages):\n",
    "    \n",
    "    driver.get(\n",
    "    f\"https://www.linkedin.com/jobs/search/?currentJobId=2662929045&geoId=106057199&keywords={cargo}&location={local}\")\n",
    "    # waiting load\n",
    "    time.sleep(5)\n",
    "\n",
    "    data = {'job':[],'description':[]}\n",
    "\n",
    "    # each page show us some jobs, sometimes show 25, others 13 or 21 ¯\\_(ツ)_/¯\n",
    "    # with this knowledge I created a loop that will check how many jobs the page is listing\n",
    "    # linkedin show us 40 jobs pages, then the line below will repeat 40 times\n",
    "    for i in range(2, npages+1):\n",
    "        try:\n",
    "        # each page show us some jobs, sometimes show 25, others 13 or 21 ¯\\_(ツ)_/¯\n",
    "            jobs_lists = driver.find_element_by_class_name('jobs-search-results__list')  # here we create a list with jobs\n",
    "            jobs = jobs_lists.find_elements_by_class_name('jobs-search-results__list-item')  # here we select each job to count\n",
    "        # waiting load\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            break\n",
    "            print('isso é tudo')\n",
    "        # the loop below is for the algorithm to click exactly on the number of jobs that is showing in list\n",
    "        # in order to avoid errors that will stop the automation\n",
    "        for n in range(1, len(jobs)+1):\n",
    "            # job click\n",
    "            try: \n",
    "                driver.find_element_by_xpath(f'/html/body/div[5]/div[3]/div[3]/div[2]/div/section[1]/div/div/ul/li[{n}]/div/div/div[1]/div[2]/div[1]/a').click()\n",
    "            # waiting load\n",
    "                time.sleep(1)\n",
    "            # select job description\n",
    "                page = driver.page_source\n",
    "                soup = BeautifulSoup(page,features=\"html.parser\")\n",
    "                job_desc = soup.find('div',{'class': \"jobs-box__html-content jobs-description-content__text t-14 t-normal\"}).find_all('span')\n",
    "                \n",
    "                ul = []\n",
    "                head = []\n",
    "                \n",
    "                for span in job_desc:\n",
    "\n",
    "                    try: data['description'].append(span.text)\n",
    "                    except: data['description'].append('')\n",
    "                        \n",
    "                    try: data['job'].append(jobs[n].text.replace('\\n','|'))\n",
    "                    except: data['job'].append('')\n",
    "                    \n",
    "            except: pass\n",
    "        # click button to change the job list\n",
    "        \n",
    "        try:\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\", WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, f'/html/body/div[5]/div[3]/div[3]/div[2]/div/section[1]/div/div/section/div/ul/li[{i}]/button'))))\n",
    "            time.sleep(5)\n",
    "            driver.find_element_by_xpath( f'/html/body/div[5]/div[3]/div[3]/div[2]/div/section[1]/div/div/section/div/ul/li[{i}]/button').click()\n",
    "            time.sleep(5)\n",
    "        except:\n",
    "            print(\"Last page reached\")\n",
    "            print(i)\n",
    "            break\n",
    "\n",
    "    # Creating a Dataframe with list\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def profile_Search_list(driver,cargo,npages):\n",
    "    \n",
    "    job = '%20'.join(cargo.split())\n",
    "    \n",
    "    driver.get(\n",
    "        f\"https://www.linkedin.com/search/results/people/?geoUrn=%5B%22106057199%22%5D&keywords={job}\")\n",
    "    # waiting load\n",
    "    time.sleep(3)\n",
    "        \n",
    "    links = []\n",
    "    \n",
    "\n",
    "    for i in range(2, npages+1):\n",
    "        try:\n",
    "            page = driver.page_source\n",
    "            soup = BeautifulSoup(page,features=\"html.parser\")\n",
    "            LinkList = soup.find('ul',{'class': \"reusable-search__entity-result-list list-style-none\"})\n",
    "\n",
    "            for link in LinkList.find_all('a'):\n",
    "                links.append(link.attrs['href'])\n",
    "            driver.get(\n",
    "            f\"https://www.linkedin.com/search/results/people/?geoUrn=%5B%22106057199%22%5D&keywords={job}&page={i}\")\n",
    "            # waiting load\n",
    "            time.sleep(3)\n",
    "        \n",
    "        except:\n",
    "            break\n",
    "            print('isso é tudo')\n",
    "\n",
    "    return links\n",
    "\n",
    "def get_profiles(driver,link_list):\n",
    "    \n",
    "    data = {'sobre':[],'experiencia':[],'formação':[]}\n",
    "    \n",
    "    for link in link_list:\n",
    "        driver.get(link)\n",
    "        # waiting load\n",
    "        time.sleep(8)\n",
    "        \n",
    "        try:\n",
    "            page = driver.page_source\n",
    "            soup = BeautifulSoup(page,features=\"html.parser\")\n",
    "            profile = soup.find('div',{'class': \"profile-detail\"})\n",
    "        except: \n",
    "            pass\n",
    "            \n",
    "        try:\n",
    "            sobre = profile.find('div',{'class': 'inline-show-more-text inline-show-more-text--is-collapsed mt4 t-14'}).text #sobre\n",
    "            data['sobre'].append(sobre.replace('\\n',' ').replace('  ',' '))\n",
    "        except: \n",
    "            data['sobre'].append('')\n",
    "            \n",
    "        try:\n",
    "            experiencia = profile.find('section',{'id': 'experience-section'}).text\n",
    "            data['experiencia'].append(experiencia.replace('\\n',' ').replace('  ',' '))\n",
    "        except: \n",
    "            data['experiencia'].append('')\n",
    "            \n",
    "        try:\n",
    "            formação = profile.find('section',{'id': 'education-section'}).text\n",
    "            data['formação'].append(formação.replace('\\n',' ').replace('  ',' '))\n",
    "        except: \n",
    "            data['formação'].append('')   \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = login(email,senha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last page reached\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "Fiscal = job_Search_complete(driver,'Analista de fiscal',local,npages=100)\n",
    "\n",
    "Fiscal['cargo'] = Fiscal['job'].str.split('|',expand=True)[0]\n",
    "Fiscal['empresa'] = Fiscal['job'].str.split('|',expand=True)[1]\n",
    "Fiscal['local'] = Fiscal['job'].str.split('|',expand=True)[2]\n",
    "\n",
    "Fiscal.to_excel('LinkedIn Fiscal.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last page reached\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "Contabilidade = job_Search_complete(driver,'Analista de Contabilidade',local,npages=100)\n",
    "\n",
    "Contabilidade['cargo'] = Contabilidade['job'].str.split('|',expand=True)[0]\n",
    "Contabilidade['empresa'] = Contabilidade['job'].str.split('|',expand=True)[1]\n",
    "Contabilidade['local'] = Contabilidade['job'].str.split('|',expand=True)[2]\n",
    "\n",
    "Contabilidade.to_excel('LinkedIn Contabilidade.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Tributário = job_Search_complete(driver,'Analista tributário',local,npages=100)\n",
    "\n",
    "Tributário['cargo'] = Data_analytics['job'].str.split('|',expand=True)[0]\n",
    "Tributário['empresa'] = Data_analytics['job'].str.split('|',expand=True)[1]\n",
    "Tributário['local'] = Data_analytics['job'].str.split('|',expand=True)[2]\n",
    "\n",
    "Tributário.to_excel('LinkedIn Tributário.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last page reached\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "Data_scientist = job_Search_complete(driver,'Cientista de dados',local,npages=100)\n",
    "\n",
    "Data_scientist['cargo'] = Data_scientist['job'].str.split('|',expand=True)[0]\n",
    "Data_scientist['empresa'] = Data_scientist['job'].str.split('|',expand=True)[1]\n",
    "Data_scientist['local'] = Data_scientist['job'].str.split('|',expand=True)[2]\n",
    "\n",
    "Data_scientist.to_excel('LinkedIn Data_scientist.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
